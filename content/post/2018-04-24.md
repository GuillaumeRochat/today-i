+++
date = "2018-04-24T20:13:42-04:00"
title = "NodeJS Streams"

+++
... played with streams in NodeJS. I had many files that I needed to merge to a single file. There are some old packages on npm to do that, but they did not seem very good. The brute force way would have been to simply read all files, concatenate them together, then write them in another last file. That works and is easy to do, but also highly inefficient as you end up having to load all the files in memory at once.

Instead of doing that, I explored the stream approach. The concept with streams in NodeJS is no different than events, with callbacks on specific events, with the added method/event pipe. It makes it possible to pipe streams together to create a flow of data, with callbacks in between. It's usable for all sorts of transformation, such as parsing a CSV file to a JSON structure. You stream the CSV file and every time it got a full line it transforms it to JSON then send the data higher in the stream. The concept of using events is only to chain the callbacks to specific moments on when to push data higher up the stream.

They came up with a simpler syntax for streams, so rather than writing down a la event listener with .on('my_event'), you can just stream.pipe(callback) and the stream object knows when to pipe the data. It's actually an interface you have to follow. So for instance, the fs package has the createReadStream and createWriteStream methods. They return ReadableStreams and WritableStreams, both objects respecting their corresponding stream interface. When you pipe a readable streams, it automatically starts reading. For instance, doing a fs.createReadStream('my-file').pipe(callback) will automatically start reading my-file and send all the bytes to callback. For CSV, I've used it to create a readStream of a CSV file that would be piped to a transform that would get the resulting JavaScript object and stringify it to a JSON string, and then the resulting JSON string would be piped to a writeStream to a new file. This way, the CSV is never entirely loaded into ram, only one object is at a time.

For what I was doing today, instead I created a single writeStream for the destination file, and then I looped over all the files I wanted to merge together, creating a readStream for each one of them. Then I piped them all to the writeStream and ended up with a single file, in a few lines of code, without ever reading all the files to ram at once.