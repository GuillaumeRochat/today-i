+++
date = "2018-02-05T20:05:36+00:00"
title = "Raft Consensus Algorithm"

+++
... read in more details about the [Raft Consensus Algorithm](https://en.wikipedia.org/wiki/Raft_(computer_science)). A difficult problem in computer science is with distributed systems. How do you make sure the system is truly fault tolerant and work exactly the same whether some parts of it fail? The concept of consensus is to have multiple systems come to an agreement on a single data value.

The Raft consensus algorithm, really well illustrated [here](https://raft.github.io/), differs from its predecessor, [Paxos](https://en.wikipedia.org/wiki/Paxos_(computer_science)), by how more easily it is to explain. The performance and fault-tolerance are both similar otherwise. One additional point for Raft is that Raft has been formally proven, which makes it very reliable. The concept of Raft is put in place in many orchestrators and tools used in clustering, such as [Nomad](https://www.nomadproject.io/) and [Consul](https://www.consul.io/). The algorithm has two different parts, the Leader Election and the Log Replication.

The Leader Election works as follow. Each node is either a follower, a candidate or a leader. When a node enters the network, it always starts as a follower. When a node is up, it has a timer of random duration and expects a leader to have communicated with it (hearth beat) before the timer runs out. Whenever the timer runs out without any leader, the node becomes a candidate and asks every other node for a vote. When a node receives a request for a vote, if it is neither a candidate nor a leader, it gives the vote. As soon as a candidate has a majority of vote, it becomes the leader and starts broadcasting a hearth beat to every other node. In the event of a draw, the nodes wait until their timer runs out again, and new candidates ask for votes. Since the timers are of random duration, it is highly unlikely that two nodes will become candidate at the exact same time, and the worst that could happen in such situation is a draw.

The Log Replication is the data storage. Whenever a client of the system wants to make a change, he asks the Leader to perform the change. The Leader tells every node that it will commit to change the data as asked by the client. Once a majority of node replied with an acknowledgement of the commit he's about to do, he does so, tells the nodes, and reply to the client that the change has been done. The nodes then commit themselves and the state has been replicated throughout the network.

The interesting part is in the event of a failure. Whenever a leader node would die, a new Leader Election would take place. What if the data was not committed in the node that became the candidate? The way it works, is with the votes comes data regarding the latest commit that took place from every other node. As soon as the candidate sees that he does not have the latest commit, he steps back to a follower, until a node with all the commit becomes the leader. Since the leader commits only once he has a majority of replies, and a candidate becomes a leader only once he has a majority of votes, and the votes include data regarding the latest log, we are guaranteed to never elect a leader  at a later state.

It is also robust in the even of a disconnection between nodes. For instance, what if two nodes including the leader would be separated from the three other nodes? The three other nodes would elect a new leader, and the new leader would be able to commit. The older leader wouldn't, as he would never get a majority of replies from the nodes. The state could not have changed between the time the disconnection occurred and a new leader was elected. So even with network failures, we are guaranteed a valid state.