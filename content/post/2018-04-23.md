+++
date = "2018-04-23T20:38:47-04:00"
title = "JavaScript Async/Await Gotcha"

+++
... learned a few things about JavaScript and mostly the use of async/await. Async/await is a very nice way to write asynchronous code with synchronous syntax. The basic is to see it just like Promise syntax without the Promise words. For instance, whenever you declare a method async, it's as if you were doing a return new Promise(); with your code inside that promise, with a default resolve for whatever the function, and a reject for whatever exception happens. The other side if when you call this method. Rather than writing method().then(result => {}), you do a const result = await method();. It's a kind of wrapper that binds whatever comes after the await in a callback given to the then method. It reads much more nicely than promises, but it also has a few gotcha.

For instance, one I hit today, is that you can do a forEach with an async callback, sure, but the forEach will just start everything and will never wait for the async callback to finish before calling the next callback. It means it's unsafe to use this method because an array of 100k entries will start 100k loops at once. On the other hand, using the for ... of syntax won't cause any problem, because it's not calling an async function, so it blocks on awaits for real. It's the equivalent on a Promise.map with a concurrency of 1 with the library Bluebird. It's however an undesirable syntax based on the airbnb-base for eslint, so the prefered method is indeed Promise.map with a concurrency of 1.

Another gotcha I had today, which is not entirely related to async/await is upon calling a method that would be doing a lot of processing for a long time. Eventually, it would break with a memory error (also because it would do everything at once in the aforementioned forEach). It however made me think about long blocking process in NodeJS. One of the very intensive method that I believed was the cause of the problem was a load of sha512 hasing on 100k objects. That sort of calculation can take a lot of time, is blocking, and therefore likely to block the whole process until it's done. Many external libraries perform this with IO, such as when you use bcryptjs, so that others can still call the process and get answers even while hundreds of people at login all at once. However, crypto isn't async, so when we hash we wait. Sure, sha512 is not that slow, it's definitely slower than sha256, but not as slow as password hashing algorithms are, so it might not be such a big deal. But what if someone were to send 1M entries of data we'd have to hash? Would the NodeJS loop eventually get tired of never getting a yield that it would just kill the process? Would external system believe the application is dead because it's not answering to small calls? How can we work around this? One thing is to not do everything at once and instead write  the code so it does it gradually. Another is to fork the process which is highly unusual in NodeJS, just to have this one thing singled out and not block the process of the NodeJS system.

All in all, both async/await and blocking need to clearly be thought out well while coding. It forces to write the program in a way that prevents blocking and prevents yielding too much on both sides.